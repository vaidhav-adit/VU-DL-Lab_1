{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "720e8a83-7d20-4158-97f3-7687f441c8a3",
   "metadata": {},
   "source": [
    "<h1>Tensor Definition: </h1>\n",
    "In deep learning, a Tensor is a multi-dimensional array of numbers. It is the fundamental data structure used by frameworks like PyTorch and TensorFlow. While a matrix is a 2D grid, a tensor can have any number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e6d50e-92c3-4555-9fd0-3f7321005b2a",
   "metadata": {},
   "source": [
    "<h2>Rank (ndims): </h2> The number of axes/dimensions. A color image is typically a Rank-3 tensor (Height, Width, Color Channels).Shape: A tuple of integers that describes how many elements the tensor has along each axis.Example: An image of $28 \\times 28$ pixels has a shape of (28, 28).Data Type (dtype): Tensors usually contain float32, int64, or uint8. Deep learning models almost exclusively use float32 or float16 for gradients and weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e516e9-40f5-493e-bf96-a1c971a69016",
   "metadata": {},
   "source": [
    "<h1>Tensor Operations</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6865f2-6465-4abf-a3da-e5c26c77ac2f",
   "metadata": {},
   "source": [
    "<h3>Creation and Initialization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bbf232b-f6cc-4a7f-9241-e26bd90abfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0624aac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "l1 = [3,4,5] #way 1 of converting a list to  tensor\n",
    "test1 = torch.tensor(l1)\n",
    "print(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "990a17f2-b0da-4c40-ade4-0a0c33313d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1, 2, 3]) #way 2 of converting a list to  tensor\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10bd2f0a-00b1-4efc-a035-db57b401f4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6393,  0.0827,  1.5868,  1.7748],\n",
       "        [ 0.1146, -0.6380,  0.3269, -0.7801],\n",
       "        [ 0.6129, -0.7639,  0.7324, -0.6224]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.randn(3, 4) #create a tensor of dimension 3x4\n",
    "t2 #(random numbers samples from normal dist with mean = 0 and var =1)\n",
    "#we can use this to assign initial random weights to the neurons in the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d39e5e22-08ef-4084-86d3-4f271a3cdc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros = torch.zeros(4, 4)\n",
    "zeros #to assign weight 0 to all the NN initally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd83b4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = torch.ones(3,3)\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3674c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'twoes'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m two = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtwoes\u001b[49m(\u001b[32m3\u001b[39m,\u001b[32m3\u001b[39m)\n\u001b[32m      2\u001b[39m two\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MML_lab_VU_-main/pytorch_env/lib/python3.12/site-packages/torch/__init__.py:2757\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m   2754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[32m   2755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[34m__name__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2757\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch' has no attribute 'twoes'"
     ]
    }
   ],
   "source": [
    "two = torch.twos(3,3)\n",
    "two # there is no TWO's in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "212d34d3-2c21-4bb7-a533-c4028a4a6e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6393,  0.0827,  1.5868,  1.7748],\n",
       "        [ 0.1146, -0.6380,  0.3269, -0.7801],\n",
       "        [ 0.6129, -0.7639,  0.7324, -0.6224]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3644571a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 5, 5, 5, 5, 5],\n",
       "        [5, 5, 5, 5, 5, 5],\n",
       "        [5, 5, 5, 5, 5, 5],\n",
       "        [5, 5, 5, 5, 5, 5],\n",
       "        [5, 5, 5, 5, 5, 5],\n",
       "        [5, 5, 5, 5, 5, 5]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five = torch.full((6, 6), 5)\n",
    "five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ca5fddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape #to tell dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41b18611-7e8e-4740-8a04-3b60188d1bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_reshaped = torch.reshape(t2, (4,3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3614e43f-e682-45a2-a60e-e947b2a702cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6393,  0.0827,  1.5868],\n",
       "        [ 1.7748,  0.1146, -0.6380],\n",
       "        [ 0.3269, -0.7801,  0.6129],\n",
       "        [-0.7639,  0.7324, -0.6224]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_reshaped #it has put the tensor in row major manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd703ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6393,  0.0827,  1.5868,  1.7748,  0.1146, -0.6380],\n",
       "        [ 0.3269, -0.7801,  0.6129, -0.7639,  0.7324, -0.6224]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_reshaped = torch.reshape(t2, (2,6))\n",
    "t2_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5123ab1a-cba5-43c5-be20-f433ae687159",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_flattened = torch.flatten(t2) #flatten = reshape(e, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcd4c23c-5c78-4459-9628-de1904368e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6393,  0.0827,  1.5868,  1.7748,  0.1146, -0.6380,  0.3269, -0.7801,\n",
       "         0.6129, -0.7639,  0.7324, -0.6224])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_flattened #flatten makes it 12 x1 (12 or e for the actual number of elements -> n x m = e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6075318c-c373-460f-b241-9d1f60b2e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_batch = t2.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0cd2eef-0367-43b4-ac87-e5d27bf67c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_batch.shape #adds a new dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "119e16c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_reshaped = torch.reshape(t2, (4,3)) t2_batch2 = t2_batch.unsqueeze(0)\n",
    "t2_batch2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf52007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3_channels = t2.unsqueeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d473466-0c44-45ca-b3eb-72ad5f99c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_batch_channel = t2_batch.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84ce6508-0d81-42eb-841d-258d2450a6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_batch_channel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f166acfd-760c-40a3-b40d-3d3d77c559de",
   "metadata": {},
   "source": [
    "<h1>Tasks</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af232d-e268-4d3e-9538-03695ce615bc",
   "metadata": {},
   "source": [
    "<h3>Task: 1 </h3> Create a random Rank-3 tensor of shape (3, 128, 128) representing an RGB image. 2. Convert it to grayscale by averaging the channel dimension (reducing Rank 3 to Rank 2). 3. \"Flatten\" the image into a 1D vector. 4. Add a \"Batch\" dimension and Channel dimention to the start so it becomes shape (1, 1, 128, 128)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88e0780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(3, 128, 128) #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "777f6d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 128])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a650615b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.mean(t, 0) #2\n",
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73e2641a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5ec80a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16384])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = torch.flatten(t2) #3\n",
    "t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fdf4d95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 128])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4 = t2.unsqueeze(0) #4\n",
    "t4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f5745590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 128, 128])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4_batch = t4.unsqueeze(0)\n",
    "t4_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8a9116-4c1d-4b9f-b56f-cc2ca5664729",
   "metadata": {},
   "source": [
    "<h1>Data Loaders</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "092f31db-5833-48ed-b67b-92f1d3ab6b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the transformation pipeline\n",
    "transform = transforms.Compose([ \n",
    "    transforms.ToTensor(),#to make sure whatever ops happen will havppen on tensor we convert pillow arry to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,)) #mean is 0.5 and sd is 0.5 \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfd29f74-19e8-4051-9dc3-4b9cab22edae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# Download and load the training data\n",
    "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Download and load the test data\n",
    "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6846e9f2-1a63-4dfa-95a2-1ea156eb0c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True) #shuffle will make sure at each iteration the model will see different sequence of samples. \n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)# like 1,2,3 then 3,2,1 then 2,3,1 etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ae058c9-82ef-4a0b-bc64-8ff2bf672f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Image Shape: torch.Size([64, 1, 28, 28])\n",
      "Batch Label Shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Get one batch of data\n",
    "images, labels = next(iter(train_loader)) #next is the one who iterates over the images and allows to you to extract info from it\n",
    "#iter is a waiter who serves the image one by one, it does not iterate \n",
    "print(f\"Batch Image Shape: {images.shape}\") \n",
    "print(f\"Batch Label Shape: {labels.shape}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf88db6-4022-419d-b41f-46ca92fc0976",
   "metadata": {},
   "source": [
    "<h3>Task2:</h3> Print the total number of batches in train_loader. 2. Extract the first image from the first batch. 3. Display its label. 4. Calculate the total number of images in the training set using only the DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29f7947b-8bc6-4cc9-8df4-1269a060e2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Batches: 938\n"
     ]
    }
   ],
   "source": [
    "number = len(train_loader) #1\n",
    "print(f\"Total Number of Batches: {number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7781aa72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_image = images[0].shape #2\n",
    "first_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4296735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_label = labels[0] #3\n",
    "first_label #this means the class is 7, \"The neural network should treat this image as belonging to category #7.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acf51402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c89290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = len(train_loader.dataset) #4 \n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4efaac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
